
spring:
  # 使用MySQL数据库
  datasource:
    url: jdbc:mysql://192.168.1.192:3306/agriculture_db?useSSL=false&serverTimezone=Asia/Shanghai&allowPublicKeyRetrieval=true
    username: y1
    password: 082415
    driver-class-name: com.mysql.cj.jdbc.Driver
    # Hikari连接池配置
    hikari:
      connection-timeout: 30000
      maximum-pool-size: 10
      minimum-idle: 5
      idle-timeout: 60000
      max-lifetime: 1800000

  # JPA配置
  jpa:
    hibernate:
      ddl-auto: update
    show-sql: false
    properties:
      hibernate:
        format_sql: true
        dialect: org.hibernate.dialect.MySQLDialect

  # 允许循环依赖
  main:
    allow-circular-references: true
    allow-bean-definition-overriding: true
    banner-mode: off
    log-startup-info: false
  
  # 文件上传配置
  servlet:
    multipart:
      enabled: true
      max-file-size: 10MB
      max-request-size: 20MB
      location: ${java.io.tmpdir}
  
  # 禁用自动配置导入防止Spring查找默认配置文件
  config:
    import: "optional:"
  

# 日志配置
logging:
  level:
    org.springframework.security: INFO
    org.springframework.web: INFO
    org.shiguang: INFO
    # 禁用Derby相关警告
    org.apache.tomcat.util.scan.StandardJarScanner: ERROR
    # Spark和Hadoop日志级别设置为ERROR，减少不必要的日志
    org.apache.spark: ERROR
    org.apache.hadoop: ERROR
    org.spark_project: ERROR
    # 将Kafka日志级别设置为ERROR，减少噪音
    org.apache.kafka: ERROR
  config: classpath:log4j2.xml
  root: OFF

# Hadoop配置
hadoop:
  enabled: true  # 启用真实的Hadoop
  config:
    fs:
      defaultFS: hdfs://192.168.1.192:9000
    dfs:
      replication: 1
    hdfs:
      user: root
  user: root

# Hive配置
hive:
  enabled: true  # 启用真实的Hive
  jdbc:
    url: jdbc:hive2://192.168.1.192:10000/default
    username: root
    password: 
    driver: org.apache.hive.jdbc.HiveDriver

# JWT配置 - 不要在生产中使用这个密钥，应通过环境变量或外部配置注入
jwt:
  # 在生产环境中，应使用环境变量设置密钥: ${JWT_SECRET}
  secret: ${JWT_SECRET:fZ1Pn2I9wR5tG8hJ0qY3xA6vB7mC4kD2lE9oF1uH3iJ7yK5zL8xM0nN3oO6pP9qQ}
  expirationMs: 86400000  # 24小时

# Spark配置
spark:
  enabled: true
  app:
    name: 农业数据分析Spark应用
  master: spark://shiguang:7077  # 连接到Spark集群而不是本地模式
  checkpoint:
    dir: /tmp/spark-checkpoint
  streaming:
    batch-duration: 5000  # 毫秒
  config:
    # 启用Spark UI和历史服务器
    spark.ui.enabled: true
    spark.eventLog.enabled: true
    spark.eventLog.dir: file:///tmp/spark-events
    spark.history.fs.logDirectory: file:///tmp/spark-events
    # 部署模式设置
    spark.submit.deployMode: cluster
    # Java 17 兼容性设置
    spark.driver.extraJavaOptions: -Dlog4j.logger.org=ERROR -Dlog4j2.disable.jmx=true -Dorg.apache.spark.internal.Logging.outputLogger=SLF4J --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED
    spark.executor.extraJavaOptions: -Dlog4j.logger.org=ERROR -Dlog4j2.disable.jmx=true -Dorg.apache.spark.internal.Logging.outputLogger=SLF4J --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED
    # 类加载器优先级，解决依赖冲突
    spark.driver.userClassPathFirst: true
    spark.executor.userClassPathFirst: true

# Kafka配置
kafka:
  enabled: true  # 启用Kafka
  # 添加正确格式的bootstrap.servers配置
  bootstrap.servers: 192.168.1.192:9092  # Spark Kafka使用的格式
  consumer:
    group-id: agricultural-data-group
    auto-offset-reset: earliest
    key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    # 设置更高的超时时间
    request.timeout.ms: 40000
    # 其他可选配置
    max.poll.records: 500
    max.poll.interval.ms: 300000
  topics:
    sensor-data: agriculture-sensor-data
    weather-data: agriculture-weather-data
    market-data: agriculture-market-data
  producer:
    key-serializer: org.apache.kafka.common.serialization.StringSerializer
    value-serializer: org.apache.kafka.common.serialization.StringSerializer
    # 提高可靠性
    acks: 1
    retries: 3
    # 提高性能
    batch.size: 16384
    linger.ms: 1
    buffer.memory: 33554432
  streaming:
    auto-start: true  # 启用自动启动Kafka流处理